{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# # Import necessary modules\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_preprocessor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\BINF-5507-Assignments\\Assignment-1\\Scripts\\data_preprocessor.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# import all necessary libraries here\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler, StandardScaler\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# # Import necessary modules\n",
    "import data_preprocessor as dp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_preprocessor import simple_model\n",
    "\n",
    "\n",
    "\n",
    "# # 1. Load the dataset\n",
    "messy_data = pd.read_csv('../Data/messy_data.csv')\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "# Display dataset information\n",
    "# messy_data.head()\n",
    "# messy_data.info()  # The data has (8) objects columns that must be transformed into numerical or float to be used in the model\n",
    "# messy_data.describe()\n",
    "\n",
    "# Remove unwanted columns (has missing valuse > 25%)\n",
    "# cleaned_data = clean_data.dropna(subset=['x', 'd','j', 'm', 'z'], axis =1)\n",
    "\n",
    "# Get rid of redundant columns by correlation method\n",
    "# Get the summary statistics for categorical variables\n",
    "# dummy_list = messy_data.describe(include='object')\n",
    "\n",
    "#create dummy for categorical variables\n",
    "# def dummy_categorical_variables(clean_data):\n",
    "#     categorical_cols = clean_data.select_dtypes(include=['object']).columns.tolist()\n",
    "#     if categorical_cols:\n",
    "#         print(f\"Encoding categorical columns: {categorical_cols}\")\n",
    "#         clean_data = pd.get_dummies(clean_data, columns=categorical_cols, drop_first=True)\n",
    "#     return clean_data\n",
    "\n",
    "# dummy_clean_data = dummy_categorical_variables(clean_data)\n",
    "\n",
    "# Create correlation matrix\n",
    "# corr = dummy_clean_data.dropna().corr(method=\"pearson\", numeric_only=True)\n",
    "\n",
    "# Create a heatmap for correlation\n",
    "# plt.figure(figsize=(24, 15))\n",
    "# sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "# plt.title(\"Spearman Correlation Heatmap Data\")\n",
    "# plt.show()\n",
    "\n",
    "# Remove one of the data with high correlation (redundant)\n",
    "# data_corr = dummy_clean_data.dropna().corr(method=\"pearson\", numeric_only=True)\n",
    "# # Filter correlations above threshold \n",
    "# high_corr = data_corr.abs() > 0.9\n",
    "# filtered_corr = data_corr.where(high_corr).dropna(how=\"all\")\n",
    "\n",
    "# print(filtered_corr)\n",
    "# print(data_corr)\n",
    "\n",
    "# # Correct selection of numeric columns from messy_data\n",
    "# data_corr = messy_data.select_dtypes(include=[np.number]).dropna().corr(numeric_only=True)\n",
    "\n",
    "# Identify features with high correlation (>0.9)\n",
    "# high_corr_pairs = set()\n",
    "\n",
    "# for i in range(len(corr.columns)):\n",
    "#     for j in range(i):\n",
    "#         if abs(corr.iloc[i, j]) > 0.9:\n",
    "#             col_name = corr.columns[i]  # Select one feature to drop\n",
    "#             high_corr_pairs.add(col_name)\n",
    "\n",
    "# Drop one of each highly correlated feature pair\n",
    "# no_redund_data = dummy_clean_data.drop(columns=high_corr_pairs)\n",
    "\n",
    "# Display updated correlation matrix\n",
    "# print(no_redund_data.corr(method=\"pearson\"))\n",
    "\n",
    "# Check for outliers\n",
    "\n",
    "\n",
    "# Visualization to find outliers\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.countplot(x=messy_data)\n",
    "# plt.title('Category Distribution')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(messy_data(), alpha=0.5)\n",
    "# plt.title(\"Scatter plot of correlated variables\")\n",
    "# plt.xlabel(\"X\")\n",
    "# plt.ylabel(\"Y\")\n",
    "# plt.show()\n",
    "\n",
    "# # # 2. Preprocess the data\n",
    "clean_data = dp.impute_missing_values(clean_data, strategy='mean')\n",
    "print(clean_data)\n",
    "\n",
    "# # Check for duplicates\n",
    "# # messy_data.duplicated().sum()\n",
    "# # clean_data = dp.remove_duplicates(clean_data)\n",
    "# # print(clean_data)\n",
    "\n",
    "# # messy_data.describe()\n",
    "# # clean_data = dp.normalize_data(clean_data)\n",
    "# # print(clean_data)\n",
    "# # clean_data.describe()  # To compare between and after standardization\n",
    "\n",
    "# clean_data = dp.remove_redundant_features(clean_data)\n",
    "# # print(clean_data)\n",
    "\n",
    "# # # 3. Save the cleaned dataset\n",
    "clean_data.to_csv('../Data/clean_data.csv', index=False)\n",
    "\n",
    "# # # 4. Train and evaluate the model\n",
    "simple_model(clean_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
